{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug  7 16:56:56 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 536.67                 Driver Version: 536.67       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1650      WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   42C    P8               1W /  30W |     90MiB /  4096MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A     19496    C+G   ...Brave-Browser\\Application\\brave.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers[sentencepiece] datasets sacrebleu rouge_score py7zr -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting numpy<2.0.0,>=1.17 (from accelerate)\n",
      "  Downloading numpy-1.26.4-cp310-cp310-win_amd64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\desktop\\textsummarization\\textsummarization\\venv\\lib\\site-packages (from accelerate) (24.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from accelerate) (5.9.2)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\asus\\desktop\\textsummarization\\textsummarization\\venv\\lib\\site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\asus\\desktop\\textsummarization\\textsummarization\\venv\\lib\\site-packages (from accelerate) (2.0.1+cu118)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\asus\\desktop\\textsummarization\\textsummarization\\venv\\lib\\site-packages (from accelerate) (0.24.5)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\asus\\desktop\\textsummarization\\textsummarization\\venv\\lib\\site-packages (from accelerate) (0.4.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\desktop\\textsummarization\\textsummarization\\venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\asus\\desktop\\textsummarization\\textsummarization\\venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.5.0)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\desktop\\textsummarization\\textsummarization\\venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\asus\\desktop\\textsummarization\\textsummarization\\venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\asus\\desktop\\textsummarization\\textsummarization\\venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\asus\\desktop\\textsummarization\\textsummarization\\venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\asus\\desktop\\textsummarization\\textsummarization\\venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\desktop\\textsummarization\\textsummarization\\venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate) (0.4.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\desktop\\textsummarization\\textsummarization\\venv\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\desktop\\textsummarization\\textsummarization\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\desktop\\textsummarization\\textsummarization\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\desktop\\textsummarization\\textsummarization\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\desktop\\textsummarization\\textsummarization\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.7.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\asus\\desktop\\textsummarization\\textsummarization\\venv\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-0.33.0-py3-none-any.whl (315 kB)\n",
      "Downloading numpy-1.26.4-cp310-cp310-win_amd64.whl (15.8 MB)\n",
      "   ---------------------------------------- 15.8/15.8 MB 3.9 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy, accelerate\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.1\n",
      "    Uninstalling numpy-2.0.1:\n",
      "      Successfully uninstalled numpy-2.0.1\n",
      "Successfully installed accelerate-0.33.0 numpy-1.26.4\n",
      "Found existing installation: transformers 4.43.4\n",
      "Uninstalling transformers-4.43.4:\n",
      "  Successfully uninstalled transformers-4.43.4\n",
      "Found existing installation: accelerate 0.33.0\n",
      "Uninstalling accelerate-0.33.0:\n",
      "  Successfully uninstalled accelerate-0.33.0\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.44.0-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting accelerate\n",
      "  Using cached accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\desktop\\textsummarization\\textsummarization\\venv\\lib\\site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\asus\\desktop\\textsummarization\\textsummarization\\venv\\lib\\site-packages (from transformers) (0.24.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\asus\\desktop\\textsummarization\\textsummarization\\venv\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\desktop\\textsummarization\\textsummarization\\venv\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\desktop\\textsummarization\\textsummarization\\venv\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\asus\\desktop\\textsummarization\\textsummarization\\venv\\lib\\site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\desktop\\textsummarization\\textsummarization\\venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\asus\\desktop\\textsummarization\\textsummarization\\venv\\lib\\site-packages (from transformers) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\asus\\desktop\\textsummarization\\textsummarization\\venv\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\asus\\desktop\\textsummarization\\textsummarization\\venv\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: psutil in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from accelerate) (5.9.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\asus\\desktop\\textsummarization\\textsummarization\\venv\\lib\\site-packages (from accelerate) (2.0.1+cu118)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\asus\\desktop\\textsummarization\\textsummarization\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\asus\\desktop\\textsummarization\\textsummarization\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\asus\\desktop\\textsummarization\\textsummarization\\venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\asus\\desktop\\textsummarization\\textsummarization\\venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\desktop\\textsummarization\\textsummarization\\venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\desktop\\textsummarization\\textsummarization\\venv\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\desktop\\textsummarization\\textsummarization\\venv\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\desktop\\textsummarization\\textsummarization\\venv\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\desktop\\textsummarization\\textsummarization\\venv\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\desktop\\textsummarization\\textsummarization\\venv\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\asus\\desktop\\textsummarization\\textsummarization\\venv\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading transformers-4.44.0-py3-none-any.whl (9.5 MB)\n",
      "   ---------------------------------------- 9.5/9.5 MB 3.4 MB/s eta 0:00:00\n",
      "Using cached accelerate-0.33.0-py3-none-any.whl (315 kB)\n",
      "Installing collected packages: accelerate, transformers\n",
      "Successfully installed accelerate-0.33.0 transformers-4.44.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade accelerate\n",
    "!pip uninstall -y transformers accelerate\n",
    "!pip install transformers accelerate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline,set_seed\n",
    "from datasets import load_dataset,load_from_disk\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datasets import load_metric\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\Desktop\\TextSummarization\\TextSummarization\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Asus\\.cache\\huggingface\\hub\\models--google--pegasus-cnn_dailymail. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Asus\\Desktop\\TextSummarization\\TextSummarization\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\Asus\\Desktop\\TextSummarization\\TextSummarization\\venv\\lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_ckpt='google/pegasus-cnn_dailymail'\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model_pegasus=AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 14732\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 819\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 818\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_samsum=load_dataset(\"Samsung/samsum\",trust_remote_code=True)\n",
    "dataset_samsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split Lengths: [14732, 819, 818]\n",
      "Features: ['id', 'dialogue', 'summary']\n",
      "\n",
      "Dialogue:\n",
      "Eric: MACHINE!\n",
      "Rob: That's so gr8!\n",
      "Eric: I know! And shows how Americans see Russian ;)\n",
      "Rob: And it's really funny!\n",
      "Eric: I know! I especially like the train part!\n",
      "Rob: Hahaha! No one talks to the machine like that!\n",
      "Eric: Is this his only stand-up?\n",
      "Rob: Idk. I'll check.\n",
      "Eric: Sure.\n",
      "Rob: Turns out no! There are some of his stand-ups on youtube.\n",
      "Eric: Gr8! I'll watch them now!\n",
      "Rob: Me too!\n",
      "Eric: MACHINE!\n",
      "Rob: MACHINE!\n",
      "Eric: TTYL?\n",
      "Rob: Sure :)\n",
      "\n",
      "Summary:\n",
      "Eric and Rob are going to watch a stand-up on youtube.\n"
     ]
    }
   ],
   "source": [
    "split_lengths=[len(dataset_samsum[split]) for split in dataset_samsum]\n",
    "\n",
    "print(f\"Split Lengths: {split_lengths}\")\n",
    "print(f\"Features: {dataset_samsum['train'].column_names}\")\n",
    "print(\"\\nDialogue:\")\n",
    "\n",
    "print(dataset_samsum[\"test\"][1][\"dialogue\"])\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "\n",
    "print(dataset_samsum[\"test\"][1][\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(example_batch):\n",
    "    input_encodings=tokenizer(example_batch['dialogue'],max_length=1024,truncation=True)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        target_encodings=tokenizer(example_batch['summary'],max_length=128,truncation=True)\n",
    "\n",
    "    return{\n",
    "        'input_ids':input_encodings['input_ids'],\n",
    "        'attention_mask':input_encodings['attention_mask'],\n",
    "        'labels':target_encodings['input_ids']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/14732 [00:00<?, ? examples/s]c:\\Users\\Asus\\Desktop\\TextSummarization\\TextSummarization\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 14732/14732 [00:04<00:00, 3525.55 examples/s]\n",
      "Map: 100%|██████████| 819/819 [00:00<00:00, 5839.77 examples/s]\n",
      "Map: 100%|██████████| 818/818 [00:00<00:00, 5143.33 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_samsum_pt=dataset_samsum.map(convert_examples_to_features,batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 14732\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_samsum_pt['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '13728867',\n",
       " 'dialogue': 'Olivia: Who are you voting for in this election? \\r\\nOliver: Liberals as always.\\r\\nOlivia: Me too!!\\r\\nOliver: Great',\n",
       " 'summary': 'Olivia and Olivier are voting for liberals in this election. ',\n",
       " 'input_ids': [18038,\n",
       "  151,\n",
       "  2632,\n",
       "  127,\n",
       "  119,\n",
       "  6228,\n",
       "  118,\n",
       "  115,\n",
       "  136,\n",
       "  2974,\n",
       "  152,\n",
       "  10463,\n",
       "  151,\n",
       "  35884,\n",
       "  130,\n",
       "  329,\n",
       "  107,\n",
       "  18038,\n",
       "  151,\n",
       "  2587,\n",
       "  314,\n",
       "  1242,\n",
       "  10463,\n",
       "  151,\n",
       "  1509,\n",
       "  1],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [18038, 111, 34296, 127, 6228, 118, 33195, 115, 136, 2974, 107, 1]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_samsum_pt['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "seq2seq_data_collator=DataCollatorForSeq2Seq(tokenizer,model=model_pegasus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments,Trainer\n",
    "\n",
    "trainer_args=TrainingArguments(\n",
    "    output_dir='pegasus-samsum',num_train_epochs=1,warmup_steps=500,per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,weight_decay=0.01,logging_steps=10,evaluation_strategy='steps',eval_steps=500,\n",
    "    save_steps=1e12,gradient_accumulation_steps=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer=Trainer(\n",
    "    model=model_pegasus,args=trainer_args,tokenizer=tokenizer,data_collator=seq2seq_data_collator,\n",
    "    train_dataset=dataset_samsum_pt['test'],eval_dataset=dataset_samsum_pt['validation']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 376.00 MiB (GPU 0; 4.00 GiB total capacity; 6.03 GiB already allocated; 0 bytes free; 6.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32mc:\\Users\\Asus\\Desktop\\TextSummarization\\TextSummarization\\venv\\lib\\site-packages\\transformers\\trainer.py:1948\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1946\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1948\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1949\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1950\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1951\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1953\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\Desktop\\TextSummarization\\TextSummarization\\venv\\lib\\site-packages\\transformers\\trainer.py:2289\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2286\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2288\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 2289\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2292\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2293\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2294\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2295\u001b[0m ):\n\u001b[0;32m   2296\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2297\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\Asus\\Desktop\\TextSummarization\\TextSummarization\\venv\\lib\\site-packages\\transformers\\trainer.py:3359\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3357\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   3358\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3359\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[1;32mc:\\Users\\Asus\\Desktop\\TextSummarization\\TextSummarization\\venv\\lib\\site-packages\\accelerate\\accelerator.py:2159\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2157\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[0;32m   2158\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2159\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Asus\\Desktop\\TextSummarization\\TextSummarization\\venv\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\Desktop\\TextSummarization\\TextSummarization\\venv\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 376.00 MiB (GPU 0; 4.00 GiB total capacity; 6.03 GiB already allocated; 0 bytes free; 6.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "def generate_batch_sized_chunks(list_of_elements,batch_size):\n",
    "    for i in range(0,len(list_of_elements),batch_size):\n",
    "        yield list_of_elements[i:i+batch_size]\n",
    "\n",
    "def calculate_metric_on_test_ds(dataset,metric,model,tokenizer,batch_size=16,device=device,\n",
    "                                column_text=\"article\",column_summary=\"highlights\"):\n",
    "    article_batches=list(generate_batch_sized_chunks(dataset[column_text],batch_size))\n",
    "    target_batches=list(generate_batch_sized_chunks([column_summary],batch_size))\n",
    "\n",
    "    for article_batch,target_batch in tqdm(zip(article_batches,target_batches),total=len(article_batches)):\n",
    "\n",
    "        inputs=tokenizer(article_batch,max_length=1024,truncation=True,padding=\"max_length\",return_tensors=\"pt\")\n",
    "\n",
    "        summaries=model.generate(input_ids=inputs[\"input_ids\"].to(device),attention_mask=inputs[\"attention_mask\"].to(device),\n",
    "                                 length_penalty=0.8,num_beams=8,max_length=128)\n",
    "        \n",
    "\n",
    "        decoded_summaries=[tokenizer.decode(s,skip_special_tokens=True,clean_up_tokenization_spaces=True) for s in summaries]\n",
    "\n",
    "        decoded_summaries=[d.replace(\"\",\" \") for d in decoded_summaries]\n",
    "\n",
    "    \n",
    "    score=metric.compute()\n",
    "    return score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_names=[\"rouge1\",\"rouge2\",\"rougeL\",\"rougeLsum\"]\n",
    "rouge_metric=load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score=calculate_metric_on_test_ds(\n",
    "    dataset_samsum['test'][0:10],rouge_metric,trainer.model,tokenizer,batch_size=2,\n",
    "    column_text='dialogue',column_summary='summary'\n",
    ")\n",
    "\n",
    "rouge_dict=dict((rn,score[rn].mid.fmeasure) for rn in rouge_names) \n",
    "\n",
    "pd.DataFrame(rouge_dict,index=[f'pegasus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "\n",
    "model_pegasus.save_pretrained(\"pegasus-samsum-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tokenizer\n",
    "tokenizer..save_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(\"/content/tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "\n",
    "gen_kwargs={\"length_penalty\":0.8,\"num_beams\":8,\"max_length\":128}\n",
    "\n",
    "sample_text=dataset_samsum['test'][0][\"dialogue\"]\n",
    "reference=dataset_samsum['test'][0][\"summary\"]\n",
    "\n",
    "pipe=pipeline(\"summarization\",model=\"pegasus-samsum-model\",tokenizer=tokenizer)\n",
    "\n",
    "print(\"Dialogue\")\n",
    "print(sample_text)\n",
    "\n",
    "print(\"\\nReference Summary\")\n",
    "print(reference)\n",
    "\n",
    "print(\"\\nModel summary\")\n",
    "print(pipe(sample_text,**gen_kwargs)[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
